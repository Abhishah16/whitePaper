## Techniques and challenges for benchmarking methods

Outline:

* We must first define what we are benchmarking
    - recovery of cell types / clusters
	- discovery of relationships between data modalities, e.g. gene
      regulatory relationships observed between chromatin
      accessibility and gene expression
	- ...
* Strategies for benchmarking
    - simulation (and we can discuss the difficulties with simulating
      covariance structure across features and data modalities)
	- benchmarking datasets
    - real datasets:
	
		Benchmark datasets for single cell studies have largely centered around measuring sequencing depth and diversity of cell types derived from a single 		     assay of interest (e.g. scRNAseq). A benchmark dataset serves a few purposes:
		1) provides ground truth for the intended effect of exposure in a proposed study design. 
		2) provides validation for a data integration task for which a new computational method may be proposed. 
		3) Both of the above
	
		For multi-modal assays, while the intended effects can vary based on the leading biological questions, one may abstract out common data integration 		    tasks such as co-embedding, mapping or correlation, and inferring causal relationships. We distinguish data integration from further downstream 			analyses that may occur on integrated samples such as differential analysis of both assays wrt to a certain exposure.

		Both the intended effects and data integration task rely on study design that takes into account 
		1) biological and technical variability via replicates, block design, and randomization
		2) power analysis for the intended effect or data integration task
		3) Dependencies between modalities, for e.g. gene expression depending on gene regulatory element activity, requires that experiment design must 		also account for spatial and temporal elements in sampling for a given observation. 
	
		As such, no universal benchmark data scheme may suit every combination of modality, and benchmark datasets may be established for commonly used 		combinations of modalities or technologies, towards specific data integration tasks. 


    - cross-validation within study (and we can discuss issues in
      matching dimensions of latent space across folds). For this Mike
      has a lot of literature in Google Doc to include on papers that
      have performed either permutation or cross-validation to assess
      model performance.
    - cross-study validation (are relationships discovered in one
      dataset present in other datasets, potentially looking across
      single cell and bulk)

